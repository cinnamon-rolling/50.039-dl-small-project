{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39264bitvenva4d00fc0239a41138288b72205840686",
   "display_name": "Python 3.9.2 64-bit ('venv')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Small Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Architecture\n",
    "\n",
    "Data\n",
    "|\n",
    "DataLoader\n",
    "|\n",
    "Resize (224x224)\n",
    "|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Inspecting Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset directory: dataset/train/normal          \tCount: 1341\nDataset directory: dataset/train/infected/non-covid\tCount: 2530\nDataset directory: dataset/train/infected/covid  \tCount: 1345\nDataset directory: dataset/val/normal            \tCount: 8\nDataset directory: dataset/val/infected/non-covid\tCount: 8\nDataset directory: dataset/val/infected/covid    \tCount: 9\nDataset directory: dataset/test/normal           \tCount: 234\nDataset directory: dataset/test/infected/non-covid\tCount: 242\nDataset directory: dataset/test/infected/covid   \tCount: 139\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"dataset\"):\n",
    "    if len(files) > 0:\n",
    "        print(\"Dataset directory: {:30}\\tCount: {}\".format(root, len(files)))"
   ]
  },
  {
   "source": [
    "# Relevant Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"    # Print out CUDA error trackback details"
   ]
  },
  {
   "source": [
    "# Image Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "])\n",
    "\n",
    "validation_transformer = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "])"
   ]
  },
  {
   "source": [
    "# 3-classes Custom Dataset Object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Lungs Dataset that can be constructed into Train, Test, Validation dataset respectively, and select a Dataset (Normal-Infected or Covid-NonCovid) to use, based on the binary classifier implementation.\n",
    "class LungDataset3C(Dataset):\n",
    "    def __init__(self, group):\n",
    "\n",
    "        self.img_size = (150, 150)\n",
    "        \n",
    "        self.class_names = ['normal', 'covid', 'non-covid']\n",
    "        self.classes = {0: 'normal', 1: 'infected_covid', 2: 'infected_non_covid'}\n",
    "        \n",
    "        self.groups = [group]\n",
    "        \n",
    "        # Number of images in each part of the dataset\n",
    "        self.dataset_numbers = {'train_normal': 1341,\\\n",
    "                                'train_infected_covid': 1345,\\\n",
    "                                'train_infected_non_covid': 2530,\\\n",
    "                                'val_normal': 8,\\\n",
    "                                'val_infected_covid': 9,\\\n",
    "                                'val_infected_non_covid': 8,\\\n",
    "                                'test_normal': 234,\\\n",
    "                                'test_infected_covid': 139,\\\n",
    "                                'test_infected_non_covid': 242}\n",
    "        \n",
    "    def get_dataset_path(self, _class):\n",
    "        sub_path = None\n",
    "        group = self.groups[0]\n",
    "        if _class == self.classes[1]:\n",
    "            sub_path = os.path.join(\"infected\", \"covid\")\n",
    "        elif _class == self.classes[2]:\n",
    "            sub_path = os.path.join(\"infected\", \"non-covid\")\n",
    "        else:\n",
    "            sub_path = \"normal\"\n",
    "        return os.path.join(\"dataset\", group, sub_path)\n",
    "\n",
    "    def filter_dataset_numbers(self):\n",
    "        filtered_dataset_numbers_map = dict()\n",
    "        for key, value in self.dataset_numbers.items():\n",
    "            if self.groups[0] in key:\n",
    "                filtered_dataset_numbers_map[key] = value\n",
    "        return filtered_dataset_numbers_map\n",
    "\n",
    "    def describe(self):\n",
    "        filtered_dataset_numbers_map = self.filter_dataset_numbers()\n",
    "        # Generate description\n",
    "        msg = \"This is the Lung {} Dataset used for the Small Project Demo in the 50.039 Deep Learning class\".format(self.groups[0].upper())\n",
    "        msg += \" in Feb-March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(len(self))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"Images have been split in three groups: training, testing and validation sets.\\n\"\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for group in self.groups:\n",
    "            for _class in self.classes.values():\n",
    "                label = \"{}_{}\".format(group, _class)\n",
    "                path = self.get_dataset_path(_class)\n",
    "                msg += \" - {}, in folder {}: {} images.\\n\".format(label, path, filtered_dataset_numbers_map[label])\n",
    "        print(msg)\n",
    "    \n",
    "    def open_img(self, _class, index):\n",
    "        group = self.groups[0]\n",
    "        if _class not in self.classes.values():\n",
    "            raise ValueError(\"Input class not found! Please input: {}. Got: {}\".format(list(self.classes.values()), _class))\n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group, _class)]\n",
    "        if index < 0 or index >= max_val:\n",
    "            raise ValueError(\"Index out of range! Should be (0 ~ {}) but got {}\".format(max_val-1, index))\n",
    "        path_to_file = os.path.join(self.get_dataset_path(_class), \"{}.jpg\".format(index))\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            if self.groups[0] == \"train\":\n",
    "              img = train_transformer(img)\n",
    "            else:\n",
    "              img = validation_transformer(img)\n",
    "        img = np.asarray(img) / 255    # Normalize\n",
    "        f.close()\n",
    "        return img\n",
    "    \n",
    "    def show_img(self, _class, index):\n",
    "        # Open image\n",
    "        im = self.open_img(_class, index)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "\n",
    "    def __len__(self):\n",
    "        length = 0\n",
    "        for key, item in self.dataset_numbers.items():\n",
    "            if self.groups[0] in key:\n",
    "                  length += item\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filtered_dataset_numbers_map = self.filter_dataset_numbers()\n",
    "        first_val = int(list(filtered_dataset_numbers_map.values())[0])\n",
    "        second_val = int(list(filtered_dataset_numbers_map.values())[1])\n",
    "        if index < first_val:\n",
    "            _class = 'normal'\n",
    "            label = 0\n",
    "        elif first_val <= index < first_val + second_val:\n",
    "            _class = 'infected_covid'\n",
    "            index = index - first_val\n",
    "            label = 1\n",
    "        else:\n",
    "            _class = 'infected_non_covid'\n",
    "            index = index - first_val - second_val\n",
    "            label = 2\n",
    "        im = self.open_img(_class, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "source": [
    "# 3-classes DataLoader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5216 615 25\nThis is the Lung TRAIN Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 5216 images, of size 150 by 150.\nImages have been split in three groups: training, testing and validation sets.\nThe images are stored in the following locations and each one contains the following number of images:\n - train_normal, in folder dataset/train/normal: 1341 images.\n - train_infected_covid, in folder dataset/train/infected/covid: 1345 images.\n - train_infected_non_covid, in folder dataset/train/infected/non-covid: 2530 images.\n\nThis is the Lung TEST Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 615 images, of size 150 by 150.\nImages have been split in three groups: training, testing and validation sets.\nThe images are stored in the following locations and each one contains the following number of images:\n - test_normal, in folder dataset/test/normal: 234 images.\n - test_infected_covid, in folder dataset/test/infected/covid: 139 images.\n - test_infected_non_covid, in folder dataset/test/infected/non-covid: 242 images.\n\nThis is the Lung VAL Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \nIt contains a total of 25 images, of size 150 by 150.\nImages have been split in three groups: training, testing and validation sets.\nThe images are stored in the following locations and each one contains the following number of images:\n - val_normal, in folder dataset/val/normal: 8 images.\n - val_infected_covid, in folder dataset/val/infected/covid: 9 images.\n - val_infected_non_covid, in folder dataset/val/infected/non-covid: 8 images.\n\n"
     ]
    }
   ],
   "source": [
    "trainset_normal_infected = LungDataset3C(group=\"train\")\n",
    "testset_nomral_infected = LungDataset3C(group=\"test\")\n",
    "valset_normal_infected = LungDataset3C(group=\"val\")\n",
    "\n",
    "train_loader = DataLoader(trainset_normal_infected, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(testset_nomral_infected, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(valset_normal_infected, batch_size=8, shuffle=True)\n",
    "\n",
    "print(len(train_loader.dataset), len(test_loader.dataset), len(val_loader.dataset))\n",
    "\n",
    "train_loader.dataset.describe()\n",
    "test_loader.dataset.describe()\n",
    "val_loader.dataset.describe()"
   ]
  },
  {
   "source": [
    "# Performance Visualization\n",
    "Functions for visualizing the prediction results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_loader.dataset.class_names\n",
    "def show_images(images, labels, preds):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 8, i + 1, xticks=[], yticks=[])\n",
    "        image = image.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = image * std + mean\n",
    "        image = np.clip(image, 0., 1.)\n",
    "        plt.imshow(image)\n",
    "        col = 'green'\n",
    "        if preds[i] != labels[i]:\n",
    "            col = 'red'\n",
    "        plt.rc('axes', labelsize=14)\n",
    "        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n",
    "        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_preds(model):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    show_images(images.cpu(), labels.cpu(), preds.cpu())"
   ]
  },
  {
   "source": [
    "# Model\n",
    "Our Custom Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout=0.7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.conv2d_6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n",
    "        self.conv2d_7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1)\n",
    "        self.conv2d_8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1)\n",
    "        self.maxPool2d = nn.MaxPool2d(kernel_size=2, stride=2, dilation=1)\n",
    "        self.linear_1 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.linear_2 = nn.Linear(4096, 4096)\n",
    "        self.linear_3 = nn.Linear(4096, 1000)\n",
    "        self.linear_4 = nn.Linear(1000, 10)\n",
    "        self.linear_5 = nn.Linear(4096, 3)      # output layer\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.adaptiveAvgPool = nn.AdaptiveAvgPool2d(7)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # 1st Conv\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        # x = F.relu(self.conv2d_2(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 2nd Conv\n",
    "        x = F.relu(self.conv2d_3(x))\n",
    "        # x = F.relu(self.conv2d_4(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 3rd Conv\n",
    "        x = F.relu(self.conv2d_5(x))\n",
    "        # x = F.relu(self.conv2d_6(x))\n",
    "        # x = F.relu(self.conv2d_6(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 4th Conv\n",
    "        x = F.relu(self.conv2d_7(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 5th Conv\n",
    "        x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        x = self.maxPool2d(x)\n",
    "        \n",
    "        x = self.adaptiveAvgPool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classifier\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = self.dropout(x)\n",
    "        # x = F.relu(self.linear_3(x))\n",
    "        # x = self.dropout(x)\n",
    "        # x = F.relu(self.linear_4(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.linear_5(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-5)\n",
    "\n",
    "# To save the model\n",
    "def save_model(model):\n",
    "    save_file = 'model/small_project_model.pth'\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "\n",
    "# Train and validate\n",
    "def train(epochs):\n",
    "\n",
    "    n_epochs = epochs\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_list = []\n",
    "    validation_loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if steps % 20 == 0:\n",
    "              accuracy = 0\n",
    "\n",
    "              # Evaluation\n",
    "              model.eval()\n",
    "              for val_step, (data, target) in enumerate(test_loader):\n",
    "              \n",
    "                  data, target = data.to(device), target.to(device)\n",
    "                  val_output = model.forward(data)\n",
    "                  loss = criterion(val_output, target)\n",
    "                  valid_loss += loss.item()\n",
    "\n",
    "                  _, preds = torch.max(val_output, 1)\n",
    "                  accuracy += sum((preds == target).cpu().numpy())\n",
    "\n",
    "              valid_loss /= (val_step + 1)\n",
    "              accuracy = accuracy / len(test_loader.dataset)\n",
    "              print(\"Epoch: {:3}/{:3} Steps: {:3}/{:3} Validation Loss: {:.6f} Accuracy: {:.4f}\".format(epoch, n_epochs, steps, len(train_loader), valid_loss, accuracy))\n",
    "\n",
    "              if accuracy > best_accuracy:\n",
    "                  best_accuracy = accuracy\n",
    "                  save_model(model)\n",
    "\n",
    "              # show predictions plots\n",
    "              # show_preds(model)\n",
    "\n",
    "              accuracy_list.append(accuracy)\n",
    "              validation_loss_list.append(valid_loss)\n",
    "\n",
    "              model.train()\n",
    "\n",
    "              if accuracy >= 0.98:\n",
    "                  print('Performance condition satisfied, stopping..')\n",
    "                  save_model(model)\n",
    "                  print(\"Run time: {:.3f} min\".format((time.time() - start)/60))\n",
    "                  return train_loss_list, validation_loss_list, accuracy_list\n",
    "\n",
    "            train_loss /= (steps + 1)\n",
    "            train_loss_list.append(train_loss)\n",
    "            steps += 1\n",
    "            \n",
    "    save_model(model)\n",
    "    print(\"Run time: {:.3f} min\".format((time.time() - start)/60))\n",
    "    return train_loss_list, validation_loss_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x25088 and 6272x4096)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-990e6fc0067d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5f6f741e3ddf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-69b51a7dd5de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/50.039-dl-small-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/50.039-dl-small-project/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/50.039-dl-small-project/venv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x25088 and 6272x4096)"
     ]
    }
   ],
   "source": [
    "train_loss_list, validation_loss_list, accuracy_list = train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}