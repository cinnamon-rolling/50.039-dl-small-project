{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([transforms.Resize((224,224)),transforms.RandomApply([\n",
    "        transforms.CenterCrop((180, 180)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(20, fill=(0,)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        ],0.7)\n",
    "  ])\n",
    "\n",
    "validation_transformer = transforms.Compose([transforms.Resize((224,224)),transforms.RandomApply([\n",
    "        transforms.CenterCrop((180, 180)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(20, fill=(0,)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        ],0.7)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Lungs Dataset that can be constructed into Train, Test, Validation dataset respectively, and select a Dataset (Normal-Infected or Covid-NonCovid) to use, based on the binary classifier implementation.\n",
    "class LungDataset3C(Dataset):\n",
    "    def __init__(self, group):\n",
    "\n",
    "        self.img_size = (150, 150)\n",
    "        \n",
    "        self.class_names = ['normal', 'covid', 'non-covid']\n",
    "        self.classes = {0: 'normal', 1: 'infected_covid', 2: 'infected_non_covid'}\n",
    "        \n",
    "        self.groups = [group]\n",
    "        \n",
    "        # Number of images in each part of the dataset\n",
    "        self.dataset_numbers = {'train_normal': 1341,\\\n",
    "                                'train_infected_covid': 1345,\\\n",
    "                                'train_infected_non_covid': 2530,\\\n",
    "                                'val_normal': 8,\\\n",
    "                                'val_infected_covid': 9,\\\n",
    "                                'val_infected_non_covid': 8,\\\n",
    "                                'test_normal': 234,\\\n",
    "                                'test_infected_covid': 139,\\\n",
    "                                'test_infected_non_covid': 242}\n",
    "        \n",
    "    def get_dataset_path(self, _class):\n",
    "        sub_path = None\n",
    "        group = self.groups[0]\n",
    "        if _class == self.classes[1]:\n",
    "            sub_path = os.path.join(\"infected\", \"covid\")\n",
    "        elif _class == self.classes[2]:\n",
    "            sub_path = os.path.join(\"infected\", \"non-covid\")\n",
    "        else:\n",
    "            sub_path = \"normal\"\n",
    "        return os.path.join(\"dataset\", group, sub_path)\n",
    "\n",
    "    def filter_dataset_numbers(self):\n",
    "        filtered_dataset_numbers_map = dict()\n",
    "        for key, value in self.dataset_numbers.items():\n",
    "            if self.groups[0] in key:\n",
    "                filtered_dataset_numbers_map[key] = value\n",
    "        return filtered_dataset_numbers_map\n",
    "\n",
    "    def describe(self):\n",
    "        filtered_dataset_numbers_map = self.filter_dataset_numbers()\n",
    "        # Generate description\n",
    "        msg = \"This is the Lung {} Dataset used for the Small Project Demo in the 50.039 Deep Learning class\".format(self.groups[0].upper())\n",
    "        msg += \" in Feb-March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(len(self))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"Images have been split in three groups: training, testing and validation sets.\\n\"\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for group in self.groups:\n",
    "            for _class in self.classes.values():\n",
    "                label = \"{}_{}\".format(group, _class)\n",
    "                path = self.get_dataset_path(_class)\n",
    "                msg += \" - {}, in folder {}: {} images.\\n\".format(label, path, filtered_dataset_numbers_map[label])\n",
    "        print(msg)\n",
    "    \n",
    "    def open_img(self, _class, index):\n",
    "        group = self.groups[0]\n",
    "        if _class not in self.classes.values():\n",
    "            raise ValueError(\"Input class not found! Please input: {}. Got: {}\".format(list(self.classes.values()), _class))\n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group, _class)]\n",
    "        if index < 0 or index >= max_val:\n",
    "            raise ValueError(\"Index out of range! Should be (0 ~ {}) but got {}\".format(max_val-1, index))\n",
    "        path_to_file = os.path.join(self.get_dataset_path(_class), \"{}.jpg\".format(index))\n",
    "        imgs = []\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            if self.groups[0] == \"train\":\n",
    "              for _ in range(10):\n",
    "                img = train_transformer(img)\n",
    "                image = np.asarray(img) / 255\n",
    "                image = transforms.functional.to_tensor(np.array(image)).float()\n",
    "                imgs.append(image)\n",
    "            else:\n",
    "              img = validation_transformer(img)\n",
    "              image = np.asarray(img) / 255\n",
    "              image = transforms.functional.to_tensor(np.array(image)).float()\n",
    "              imgs.append(image)\n",
    "        f.close()\n",
    "        return imgs\n",
    "    \n",
    "    def show_img(self, _class, index):\n",
    "        # Open image\n",
    "        im = self.open_img(_class, index)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "\n",
    "    def __len__(self):\n",
    "        length = 0\n",
    "        for key, item in self.dataset_numbers.items():\n",
    "            if self.groups[0] in key:\n",
    "                  length += item\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filtered_dataset_numbers_map = self.filter_dataset_numbers()\n",
    "        first_val = int(list(filtered_dataset_numbers_map.values())[0])\n",
    "        second_val = int(list(filtered_dataset_numbers_map.values())[1])\n",
    "        if index < first_val:\n",
    "            _class = 'normal'\n",
    "            labels = 0\n",
    "        elif first_val <= index < first_val + second_val:\n",
    "            _class = 'infected_covid'\n",
    "            index = index - first_val\n",
    "            labels = 1\n",
    "        else:\n",
    "            _class = 'infected_non_covid'\n",
    "            index = index - first_val - second_val\n",
    "            labels = 2\n",
    "        imgs = self.open_img(_class, index)\n",
    "        return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f3bec0c9e10> <torch.utils.data.dataloader.DataLoader object at 0x7f3bec0c9ef0> <torch.utils.data.dataloader.DataLoader object at 0x7f3bec0c9f28>\n",
      "5216 615 25\n",
      "This is the Lung TRAIN Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \n",
      "It contains a total of 5216 images, of size 150 by 150.\n",
      "Images have been split in three groups: training, testing and validation sets.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - train_normal, in folder dataset/train/normal: 1341 images.\n",
      " - train_infected_covid, in folder dataset/train/infected/covid: 1345 images.\n",
      " - train_infected_non_covid, in folder dataset/train/infected/non-covid: 2530 images.\n",
      "\n",
      "This is the Lung TEST Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \n",
      "It contains a total of 615 images, of size 150 by 150.\n",
      "Images have been split in three groups: training, testing and validation sets.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - test_normal, in folder dataset/test/normal: 234 images.\n",
      " - test_infected_covid, in folder dataset/test/infected/covid: 139 images.\n",
      " - test_infected_non_covid, in folder dataset/test/infected/non-covid: 242 images.\n",
      "\n",
      "This is the Lung VAL Dataset used for the Small Project Demo in the 50.039 Deep Learning class in Feb-March 2021. \n",
      "It contains a total of 25 images, of size 150 by 150.\n",
      "Images have been split in three groups: training, testing and validation sets.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - val_normal, in folder dataset/val/normal: 8 images.\n",
      " - val_infected_covid, in folder dataset/val/infected/covid: 9 images.\n",
      " - val_infected_non_covid, in folder dataset/val/infected/non-covid: 8 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainset_normal_infected = LungDataset3C(group=\"train\")\n",
    "testset_nomral_infected = LungDataset3C(group=\"test\")\n",
    "valset_normal_infected = LungDataset3C(group=\"val\")\n",
    "\n",
    "train_loader = DataLoader(trainset_normal_infected, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset_nomral_infected, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valset_normal_infected, batch_size=32, shuffle=True)\n",
    "\n",
    "print(train_loader, test_loader, val_loader)\n",
    "print(len(train_loader.dataset), len(test_loader.dataset), len(val_loader.dataset))\n",
    "\n",
    "train_loader.dataset.describe()\n",
    "test_loader.dataset.describe()\n",
    "val_loader.dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_loader.dataset.class_names\n",
    "def show_images(images, labels, preds):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(4, 8, i + 1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.squeeze(image))\n",
    "        col = 'green'\n",
    "        if preds[i] != labels[i]:\n",
    "            col = 'red'\n",
    "        plt.rc('axes', labelsize=14)\n",
    "        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n",
    "        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_preds(model):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[0].to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    show_images(images.cpu(), labels.cpu(), preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_val_to_csv(val, name):\n",
    "    \n",
    "    with open(\"{}.csv\".format(name), \"a\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"{}\\n\".format(val))\n",
    "    fh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout=0.7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2d_8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxPool2d = nn.MaxPool2d(kernel_size=2, stride=2, dilation=1)\n",
    "        self.linear_1 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.linear_2 = nn.Linear(4096, 4096)\n",
    "        self.linear_3 = nn.Linear(4096, 1000)\n",
    "        self.linear_4 = nn.Linear(1000, 3)\n",
    "        # self.linear_5 = nn.Linear(4096, 3)      # output layer\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.adaptiveAvgPool = nn.AdaptiveAvgPool2d(7)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # 1st Conv\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        # x = F.relu(self.conv2d_2(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 2nd Conv\n",
    "        x = F.relu(self.conv2d_3(x))\n",
    "        # x = F.relu(self.conv2d_4(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 3rd Conv\n",
    "        x = F.relu(self.conv2d_5(x))\n",
    "        x = F.relu(self.conv2d_6(x))\n",
    "        # x = F.relu(self.conv2d_6(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 4th Conv\n",
    "        x = F.relu(self.conv2d_7(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        # 5th Conv\n",
    "        x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        # x = F.relu(self.conv2d_8(x))\n",
    "        x = self.maxPool2d(x)\n",
    "\n",
    "        x = self.adaptiveAvgPool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classifier\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear_4(x))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.linear_5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Check if trained model existed, if yes load it\n",
    "if os.path.exists(\"./small_project_model_2.pth\"):\n",
    "    model.load_state_dict(torch.load(\"./small_project_model_2.pth\"))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-5)\n",
    "\n",
    "def save_model(model):\n",
    "    save_file = 'small_project_model_2.pth'\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "\n",
    "def train(epochs):\n",
    "\n",
    "    n_epochs = epochs\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        train_loss = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for datas, target in train_loader:\n",
    "            for data in datas:\n",
    "              data, tmp_target = data.cuda(), target.cuda()\n",
    "              optimizer.zero_grad()\n",
    "              output = model.forward(data)\n",
    "              loss = criterion(output, tmp_target)\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "              train_loss += loss.item()\n",
    "\n",
    "              if steps % 100 == 0:\n",
    "                accuracy = 0\n",
    "\n",
    "                # Evaluation\n",
    "                valid_loss = 0\n",
    "                model.eval()\n",
    "                for val_step, (data, val_target) in enumerate(test_loader):\n",
    "                \n",
    "                    data, tmp_target = data[0].cuda(), val_target.cuda()\n",
    "                    val_output = model.forward(data)\n",
    "                    loss = criterion(val_output, tmp_target)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(val_output, 1)\n",
    "                    accuracy += sum((preds == tmp_target).cpu().numpy())\n",
    "\n",
    "                valid_loss /= (val_step + 1)\n",
    "                accuracy = accuracy / len(test_loader.dataset)\n",
    "                train_loss /= (steps + 1)\n",
    "                \n",
    "                print(\"Epoch: {:3}/{:3} Steps: {:4}/{:4} Train Loss: {:.6f} Validation Loss: {:.6f} Accuracy: {:.4f}\".format(epoch, n_epochs, steps, len(train_loader)*10, train_loss, valid_loss, accuracy))\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    save_model(model)\n",
    "\n",
    "                # show predictions plots\n",
    "                # show_preds(model)\n",
    "\n",
    "                write_val_to_csv(valid_loss, \"valid_loss\")\n",
    "                write_val_to_csv(accuracy, \"accuracy\")\n",
    "                write_val_to_csv(train_loss, \"train_loss\")\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                if accuracy >= 0.98:\n",
    "                    print('Performance condition satisfied, stopping..')\n",
    "                    save_model(model)\n",
    "                    print(\"Run time: {:.3f} min\".format((time.time() - start)/60))\n",
    "                    return train_loss_list, validation_loss_list, accuracy_list\n",
    "\n",
    "              steps += 1          \n",
    "            \n",
    "    save_model(model)\n",
    "    print(\"Run time: {:.3f} min\".format((time.time() - start)/60))\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 12 Steps:    0/1630 Train Loss: 0.430239 Validation Loss: 0.578710 Accuracy: 0.8228\n",
      "Epoch:   1/ 12 Steps:  100/1630 Train Loss: 0.703046 Validation Loss: 1.450348 Accuracy: 0.5593\n",
      "Epoch:   1/ 12 Steps:  200/1630 Train Loss: 0.381256 Validation Loss: 0.803663 Accuracy: 0.7626\n",
      "Epoch:   1/ 12 Steps:  300/1630 Train Loss: 0.245084 Validation Loss: 0.686093 Accuracy: 0.7626\n",
      "Epoch:   1/ 12 Steps:  400/1630 Train Loss: 0.188712 Validation Loss: 0.572547 Accuracy: 0.8163\n",
      "Epoch:   1/ 12 Steps:  500/1630 Train Loss: 0.150108 Validation Loss: 0.761366 Accuracy: 0.7642\n",
      "Epoch:   1/ 12 Steps:  600/1630 Train Loss: 0.131382 Validation Loss: 0.645203 Accuracy: 0.8016\n",
      "Epoch:   1/ 12 Steps:  700/1630 Train Loss: 0.101853 Validation Loss: 0.721362 Accuracy: 0.7967\n",
      "Epoch:   1/ 12 Steps:  800/1630 Train Loss: 0.091960 Validation Loss: 0.719461 Accuracy: 0.7691\n",
      "Epoch:   1/ 12 Steps:  900/1630 Train Loss: 0.080508 Validation Loss: 0.848460 Accuracy: 0.7642\n",
      "Epoch:   1/ 12 Steps: 1000/1630 Train Loss: 0.069929 Validation Loss: 0.555849 Accuracy: 0.8325\n",
      "Epoch:   1/ 12 Steps: 1100/1630 Train Loss: 0.070940 Validation Loss: 0.680008 Accuracy: 0.7805\n",
      "Epoch:   1/ 12 Steps: 1200/1630 Train Loss: 0.059878 Validation Loss: 0.730689 Accuracy: 0.7659\n",
      "Epoch:   1/ 12 Steps: 1300/1630 Train Loss: 0.056447 Validation Loss: 0.605730 Accuracy: 0.8163\n",
      "Epoch:   1/ 12 Steps: 1400/1630 Train Loss: 0.053409 Validation Loss: 0.695977 Accuracy: 0.7902\n",
      "Epoch:   1/ 12 Steps: 1500/1630 Train Loss: 0.046188 Validation Loss: 0.879979 Accuracy: 0.7382\n",
      "Epoch:   1/ 12 Steps: 1600/1630 Train Loss: 0.044576 Validation Loss: 0.574918 Accuracy: 0.8114\n",
      "Epoch:   2/ 12 Steps:    0/1630 Train Loss: 0.460892 Validation Loss: 0.755438 Accuracy: 0.7821\n",
      "Epoch:   2/ 12 Steps:  100/1630 Train Loss: 0.721185 Validation Loss: 0.667181 Accuracy: 0.8016\n",
      "Epoch:   2/ 12 Steps:  200/1630 Train Loss: 0.393826 Validation Loss: 0.687061 Accuracy: 0.7463\n",
      "Epoch:   2/ 12 Steps:  300/1630 Train Loss: 0.248216 Validation Loss: 0.759413 Accuracy: 0.7366\n",
      "Epoch:   2/ 12 Steps:  400/1630 Train Loss: 0.183326 Validation Loss: 0.782706 Accuracy: 0.7333\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 8.00 GiB total capacity; 5.90 GiB already allocated; 18.79 MiB free; 6.49 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-16e1904b655d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9492fbe68ea8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0mval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-802107c5f602>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 3rd Conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# x = F.relu(self.conv2d_6(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 8.00 GiB total capacity; 5.90 GiB already allocated; 18.79 MiB free; 6.49 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_loss_list, validation_loss_list, accuracy_list = train(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_loader.dataset[0]\n",
    "labels = [labels for _ in range(len(images))]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.squeeze(image))\n",
    "    col = 'green'\n",
    "    plt.rc('axes', labelsize=14)\n",
    "    plt.xlabel(f'{class_names[int(labels[i])]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV for data recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(csv_path):\n",
    "    val_list = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip(\"\\n\")\n",
    "            if len(line) > 0:\n",
    "                val_list.append(float(line))\n",
    "    return val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Graph for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy_list = retrieve_data(\"accuracy.csv\")\n",
    "steps = len(accuracy_list)\n",
    "plt.plot(np.arange(1, steps+1,1),accuracy_list[:steps], label='accuracy')\n",
    "plt.xticks(range(1,steps+1,2))\n",
    "plt.xlim(1,steps+1)\n",
    "plt.xlabel('steps (20 batches)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "filename = 'accuracy.png'\n",
    "plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Graph for Train, Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = retrieve_data(\"train_loss.csv\")\n",
    "validation_loss_list = retrieve_data(\"valid_loss.csv\")\n",
    "steps = len(validation_loss_list)\n",
    "plt.plot(np.arange(1, steps+1,1),train_loss_list[:steps], label='average train loss')\n",
    "plt.plot(np.arange(1,steps+1,1), validation_loss_list[:steps], label='average validation loss')\n",
    "plt.xticks(range(1,steps+1,2))\n",
    "plt.xlim(1,steps+1)\n",
    "plt.xlabel('steps (20 batches)')\n",
    "plt.ylabel('running losses')\n",
    "plt.title('loss reduction')\n",
    "plt.legend(loc='upper right')\n",
    "filename = 'loss_profile.png'\n",
    "plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
